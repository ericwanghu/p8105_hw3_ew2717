---
title: "p8105_hw3_ew2717"
author: "Eric Wang"
output: github_document
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)

```


```{r setup two}

library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

## Problem 1

*Loading instacart data*

```{r}

data("instacart")

instacart_df <-
  instacart %>% 
  janitor::clean_names()

```

#### Instacart data exploration 

*Finding rows, columns, and key variables*

```{r intstacart rows cols names}

instacart_rows = instacart_df %>% 
  nrow()
instacart_cols = instacart_df %>% 
  ncol()
instacart_names = instacart_df %>% 
  names()

```

The number of rows in the instacart data set is **`r instacart_rows`**. The number of columns in the instacart data set is **`r instacart_cols`**. The key variables in the instacart data set are **`r instacart_names`**.

*Finding how many aisles there are and which aisles are the most items ordered from*

```{r instacart aisles}

instacart_aisles <- 
  instacart_df %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  arrange(desc(n_obs))

instacart_aisles

```

As seen above, there are 134 aisles, and the aisles that have the most items ordered from are the "fresh vegetables", "fresh fruits", "packaged vegetable fruits", "yogurt", and "packaged cheese". 


*Creating a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, including the number of times each item is ordered*

```{r aisle table}

aisle_table <- 
  instacart_df %>% 
  group_by(aisle) %>% 
  summarize(item_count = n()) %>% 
  filter(item_count > 10000) %>% 
  ggplot(aes(x = reorder(aisle, item_count), 
             y = item_count)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Items ordered per Aisle",
    x = "Aisle",
    y = "# of Items",
    caption = "Figure 1. The table depicts the number of items ordered per aisle (>10000 items)."
  ) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 6))

aisle_table

```

*Making a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits", including the number of times each item is ordered*

```{r popular_items_table, message = FALSE, warning = FALSE}

popular_table <- 
  instacart_df %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(item_count = n()) %>% 
  filter(min_rank(desc(item_count)) < 4) %>% 
  arrange(desc(item_count)) 
  
knitr::kable(popular_table)

```

## PROBLEM 2 

*Loading BRFSS dataset*

```{r load brfss data}

data("brfss_smart2010")

```

*Cleaning brfss data set*

```{r clean brfss data set}

brfss_df <- 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(
  topic == "Overall Health",
  response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) %>% 
  mutate(response = as.factor(response), response = ordered(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  select(year, locationdesc, topic, response, data_value, sample_size) %>%
  separate(locationdesc, into = c("State", "County"), sep = " - ")

```

*Finding which states were observed at 7 or more locations in 2002 and 2010*

```{r observed 7 or more}

brfss_df_02 <- 
  brfss_df %>% 
  group_by(State) %>% 
  filter(year == "2002") %>%
  distinct(County) %>%
  count(State) %>% 
  filter(n >= 7) %>% 
  arrange(n)

brfss_df_10 <- 
  brfss_df %>% 
  group_by(State) %>% 
  filter(year == "2010") %>%
  distinct(County) %>%
  count(State) %>% 
  filter(n >= 7) %>% 
  arrange(n)

knitr::kable(brfss_df_02)
knitr::kable(brfss_df_10)

```

As the tables depict, CT, FL, NC, MA, NJ, and PA were observed at 7 or more locations in 2002, and CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA were observed at 7 or more locations in 2010.

*Constructing a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state* 

```{r excellent dataset}

brfss_excellent_df <-
  brfss_df %>%
  filter(response == "Excellent") %>% 
  group_by(State, year) %>% 
  mutate(avg_data_value = mean(data_value, na.rm = TRUE),
         avg_data_value = round(avg_data_value, digits = 4)) %>%
  select(year, State, avg_data_value) 

```

*Making a spaghetti plot of this average value over time within a state*

```{r spaghetti plot, warning = FALSE, message = FALSE}

brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(year, State) %>%
  summarize(mean_value = mean(data_value), na.rm = TRUE) %>% 
  ggplot(aes(x = year, 
             y = mean_value, 
             group = State, 
             color = State)) +
  geom_line() +
  labs(
    title = "Average Value over Time per State",
    x = "Year",
    y = "Average Value") +
  theme(legend.position = "right")

```

This spaghetti plot shows that the average data_value across locations within states seem to cary greatly every single year, from 2002 to 2010.

*Making a two-panel plot showing the distribution of data_value for responses among locations in NY State for the years 2006 and 2010*

```{r two panel plot}

brfss_df %>%
  filter(year == "2006" | year == "2010",
         State == "NY") %>% 
  ggplot(aes(x = response, y = data_value, color = response)) +
  geom_boxplot() +
  facet_grid(. ~ year) +
labs(
    title = "NY Distribution of data values for 2006 and 2010, by Responses",
    x = "Response",
    y = "Data Value"
  )

```

## Problem 3

*Loading original accel data set*

```{r}

accel_df <- 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() 

```

*Tidying data set*

```{r}

accel_tidy <-
  accel_df %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count") %>%
  mutate(
    week = as.integer(week), 
    day_id = as.integer(day_id), 
    minute = as.integer(minute),
    hour = as.integer(minute %/% 60), 
    day = factor(day, levels = c("Sunday","Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
    weekday_vs_weekend = 
      case_when(
           day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "Weekday", 
           day %in% c("Saturday", "Sunday") ~ "Weekend")) 

```
 
*Analyzing accelerometer data*

```{r}

accel_var = accel_tidy %>% 
  names()
accel_dim = accel_tidy %>% 
  dim()
accel_rows = accel_tidy %>% 
  nrow()
accel_cols = accel_tidy %>% 
  ncol()

```

This data set was tidied, and we found that its key variables are **`r accel_var`**. Additionally, the dimensions are **`r accel_dim`**, the number of columns are **`r accel_cols`**, and the number of rows are **`r accel_rows`**. 

*Creating a table that shows total activity count for each day of the week*

```{r}

accel_tidy %>%
  group_by(day) %>%
  summarize(total_activity = sum(activity_count, na.rm = TRUE)) %>%
  knitr::kable(caption = 
      "Figure 2. Total Activity Count per Day"
  )

```

One apparent trend seen from the table is that activity count is noticeably higher towards the end of the week, from Wednesday to Friday. Additionally, Saturday can be assumed to be a rest day, as it is significantly lower in total activity compared to the rest of the days. 

*Making a single-panel plot that depicts 24-hour activity time courses for every day*

```{r}

accel_tidy %>%
  group_by(day, minute) %>%
  ggplot(aes(x = minute, 
             y = activity_count, 
             color = day)) +
  geom_point() +
  labs(
    title = "24-Hour Activity Time per Day",
    x = "Hour in the Day",
    y = "Activity Count",
    caption = "Figure 3. The following data is from  Columbia University Medical Center") + 
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440),
    labels = c("0 hr", "4 hr", "8 hr", "12 hr", "16 hr", "20 hr", "24 hr"))

```

This accelerometer data set is tidied. It shows five weeks of data for one person.

Based on this plot, we can see that activity count is higher later on throughout the day during weekdays. On the weekends, however, we can see that activity count is actually higher throughout the middle of the day. 
